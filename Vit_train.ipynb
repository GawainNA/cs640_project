{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tvuvkZgxFgc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datasets\n",
        "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    nltk.download(\"punkt\", quiet=True)"
      ],
      "metadata": {
        "id": "nY17vx1LxTOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize VisionEncoderDecoderModel"
      ],
      "metadata": {
        "id": "aBvm7U-BxWug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "\n",
        "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "text_decode_model = \"gpt2\"\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    image_encoder_model, text_decode_model)\n",
        "\n",
        "# image feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
        "# text tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n",
        "\n",
        "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# update the model config\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "output_dir = \"vit-gpt-model\"\n",
        "model.save_pretrained(output_dir)\n",
        "feature_extractor.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr1DqrEfxebC",
        "outputId": "2bd1dfb8-3376-4ff4-bb53-d3381a993ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('vit-gpt-model/tokenizer_config.json',\n",
            "'vit-gpt-model/special_tokens_map.json',\n",
            "'vit-gpt-model/vocab.json',\n",
            "'vit-gpt-model/merges.txt',\n",
            "'vit-gpt-model/added_tokens.json',\n",
            "'vit-gpt-model/tokenizer.json')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preparation"
      ],
      "metadata": {
        "id": "3sZ5gwKix57T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wget http://images.cocodataset.org/zips/train2017.zip\n",
        "wget http://images.cocodataset.org/zips/val2017.zip\n",
        "wget http://images.cocodataset.org/zips/test2017.zip\n",
        "wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "wget http://images.cocodataset.org/annotations/image_info_test2017.zip Then to load the dataset:\n",
        "\n",
        "COCO_DIR = \"drive/MyDrive/datasets/coco_2017/\"\n",
        "ds = datasets.load_dataset(\"coco_dataset_script\", \"2017\", data_dir=COCO_DIR)"
      ],
      "metadata": {
        "id": "LHdylSjgxpjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print single example\n",
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxiV5KMuyAMZ",
        "outputId": "2d7ff186-55b8-4753-f090-05c8a23f0312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image_id': 74,\n",
            "'caption_id': 145996,\n",
            "'caption': 'A picture of a dog laying on the ground.',\n",
            "'height': 426,\n",
            "'width': 640,\n",
            "'file_name': '000000000074.jpg',\n",
            "'coco_url': 'http://images.cocodataset.org/train2017/000000000074.jpg',\n",
            "'image_path': '/.cache/huggingface/datasets/downloads/extracted/f1122be5b6fbdb4a45c67365345f5639d2e11a25094285db1348c3872189a0f6/train2017/000000000074.jpg'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# text preprocessing step\n",
        "def tokenization_fn(captions, max_target_length):\n",
        "    \"\"\"Run tokenization on captions.\"\"\"\n",
        "    labels = tokenizer(captions, \n",
        "                      padding=\"max_length\", \n",
        "                      max_length=max_target_length).input_ids\n",
        "\n",
        "    return labels\n",
        "\n",
        "# image preprocessing step\n",
        "def feature_extraction_fn(image_paths, check_image=True):\n",
        "    \"\"\"\n",
        "    Run feature extraction on images\n",
        "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
        "    Otherwise, an exception will be thrown.\n",
        "    \"\"\"\n",
        "\n",
        "    model_inputs = {}\n",
        "\n",
        "    if check_image:\n",
        "        images = []\n",
        "        to_keep = []\n",
        "        for image_file in image_paths:\n",
        "            try:\n",
        "                img = Image.open(image_file)\n",
        "                images.append(img)\n",
        "                to_keep.append(True)\n",
        "            except Exception:\n",
        "                to_keep.append(False)\n",
        "    else:\n",
        "        images = [Image.open(image_file) for image_file in image_paths]\n",
        "\n",
        "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
        "\n",
        "    return encoder_inputs.pixel_values\n",
        "\n",
        "def preprocess_fn(examples, max_target_length, check_image = True):\n",
        "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
        "    image_paths = examples['image_path']\n",
        "    captions = examples['caption']    \n",
        "    \n",
        "    model_inputs = {}\n",
        "    # This contains image path column\n",
        "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
        "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "O_D1TTkZziWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = ds.map(\n",
        "    function=preprocess_fn,\n",
        "    batched=True,\n",
        "    fn_kwargs={\"max_target_length\": 128},\n",
        "    remove_columns=ds['train'].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "vh8duAHjzpii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmf2Xx9Bc0Q7",
        "outputId": "fbf16643-6c30-4f1f-99c8-b2bf06d4f3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
            "        num_rows: 18057\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
            "        num_rows: 470\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
            "        num_rows: 4060\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define seq2seq training arguments"
      ],
      "metadata": {
        "id": "CImGyRa6zvS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=50,\n",
        "    output_dir=\"./image-captioning-output\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npanZkPzzxOW",
        "outputId": "8cb23049-198d-40c7-9464-ae473d5e0a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define metric"
      ],
      "metadata": {
        "id": "ZngaulrZ0HlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "ignore_pad_token_for_loss = True\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
        "    ]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return result"
      ],
      "metadata": {
        "id": "XprMAhs90I0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "HsXFnFCJ0XvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=feature_extractor,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=processed_dataset['train'],\n",
        "    eval_dataset=processed_dataset['validation'],\n",
        "    data_collator=default_data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "CAi9-2hs0ZbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLwVf4oy0gCE",
        "outputId": "c09fd098-20f0-42c6-df03-debc19339c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 18057\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 18057\n",
            "  Batch size = 8\n",
            "  Epoch Training Loss = 18.242900\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 18057\n",
            "  Batch size = 8\n",
            "  Epoch Training Loss = 2.052800\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 18057\n",
            "  Batch size = 8\n",
            "  Epoch Training Loss = 0.318858\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "TrainOutput(global_step=60, training_loss=0.3188589096069336, metrics={'train_runtime': 377574.1556, 'train_samples_per_second': 0.697, 'train_steps_per_second': 0.174, 'total_flos': 4.331133386883072e+16, 'train_loss': 0.3188589096069336, 'epoch': 3.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./image-captioning-output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOvKzLDrFtp-",
        "outputId": "45b47caa-70ee-4e72-d87e-29c8dc324c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model checkpoint to ./image-captioning-outputn\n",
            "Configuration saved in ./image-captioning-output/config.json\n",
            "Model weights saved in ./image-captioning-output/pytorch_model.bin\n",
            "Feature extractor saved in ./image-captioning-output/preprocessor_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"./image-captioning-output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcfcOykF63l",
        "outputId": "6c2795f9-d6fe-47ca-f570-dcd105787ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "okenizer config file saved in ./image-captioning-output/tokenizer_config.json\n",
            "Special tokens file saved in ./image-captioning-output/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "pAMpHGrDGEkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "image_captioner = pipeline(\"image-to-text\", model=\"./image-captioning-output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AJLhApeGIm7",
        "outputId": "d6776d26-a436-4e0f-cb11-9ee93a6f14b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading configuration file ./image-captioning-output/config.json\n",
            "Model config VisionEncoderDecoderConfig {\n",
            "  \"_commit_hash\": null,\n",
            "  \"_name_or_path\": \"./image-captioning-output\",\n",
            "  \"architectures\": [\n",
            "    \"VisionEncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"gpt2\",\n",
            "    \"activation_function\": \"gelu_new\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"GPT2LMHeadModel\"\n",
            "    ],\n",
            "    \"attn_pdrop\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": 50256,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"embd_pdrop\": 0.1,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 50256,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_epsilon\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"gpt2\",\n",
            "    \"n_ctx\": 1024,\n",
            "    \"n_embd\": 768,\n",
            "    \"n_head\": 12,\n",
            "    \"n_inner\": null,\n",
            "    \"n_layer\": 12,\n",
            "    \"n_positions\": 1024,\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": null,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"reorder_and_upcast_attn\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"resid_pdrop\": 0.1,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"scale_attn_by_inverse_layer_idx\": false,\n",
            "    \"scale_attn_weights\": true,\n",
            "    \"sep_token_id\": null,\n",
            "    \"summary_activation\": null,\n",
            "    \"summary_first_dropout\": 0.1,\n",
            "    \"summary_proj_to_labels\": true,\n",
            "    \"summary_type\": \"cls_index\",\n",
            "    \"summary_use_proj\": true,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": {\n",
            "      \"text-generation\": {\n",
            "        \"do_sample\": true,\n",
            "        \"max_length\": 50\n",
            "      }\n",
            "    },\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.23.1\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50257\n",
            "  },\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"ViTModel\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.0,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"encoder_stride\": 16,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.0,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"image_size\": 224,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"vit\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": null,\n",
            "    \"patch_size\": 16,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"qkv_bias\": true,\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.23.1\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false\n",
            "  },\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"model_type\": \"vision-encoder-decoder\",\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null\n",
            "}\n",
            "\n",
            "\n",
            "loading weights file ./image-captioning-output/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing VisionEncoderDecoderModel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_captioner(\"sample_image.png\")"
      ],
      "metadata": {
        "id": "ofntXTWFIbmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}