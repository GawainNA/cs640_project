{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Evauation Frame on Text-to-Image Model\n",
        "\n",
        "This frame is used to evaluate text-to-image models. With Input Text and Generated Images, it will give a score between 0 ~ 5, 0 means the worst. This frame has three components, \"Object Detection\", \"Image Captioning\" and \"Sentence Analysis\". This notebook select \"Mask R-CNN\", \"ViT-GPT2\" and \"LaBSE\" as an example combination to build this frame in sequence."
      ],
      "metadata": {
        "id": "JGUSVUU5DCTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Dependencies\n",
        "In this part we will first install all the requirements and import needed packages"
      ],
      "metadata": {
        "id": "Zvz-5iAwD36E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5v_LxW0C-Iq"
      },
      "outputs": [],
      "source": [
        "# transformers and flax\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install flax\n",
        "!pip install --upgrade git+https://github.com/google/flax.git\n",
        "!apt-cache policy libcudnn8\n",
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6\n",
        "!export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install detectron2 \n",
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ],
      "metadata": {
        "id": "Kqj3BgbPE6MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Installation, now it's time to do some setup"
      ],
      "metadata": {
        "id": "2leENKbNGGFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "\n",
        "import requests\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from transformers import ViTFeatureExtractor, AutoTokenizer, FlaxVisionEncoderDecoderModel\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "import torch, detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "2zlk8mCyFvra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Process"
      ],
      "metadata": {
        "id": "ZEMR7Cd1Gr27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Data setup"
      ],
      "metadata": {
        "id": "ugff1yE2HGah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated Input image from Text-to-Image model\n",
        "input_image_path = \"\"\n",
        "\n",
        "# Input Text Prompt\n",
        "input_text = \"\""
      ],
      "metadata": {
        "id": "oEcKoAY3HORo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Image captioning"
      ],
      "metadata": {
        "id": "iCJWmzUPIEEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we will use the Hugging Face transformers' Vision-To-Text Encoder-Decoder framework with the encoder Vision Transformer and decoder GPT2, to generate image caption."
      ],
      "metadata": {
        "id": "DmB_wHf-P6Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image captioning model\n",
        "loc = \"ydshieh/vit-gpt2-coco-en\"\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n",
        "tokenizer = AutoTokenizer.from_pretrained(loc)\n",
        "image_cap_model = FlaxVisionEncoderDecoderModel.from_pretrained(loc)"
      ],
      "metadata": {
        "id": "eral64DoISZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function of predict image caption\n",
        "def generate_caption(image_path, model):\n",
        "  with Image.open(image_path) as img:\n",
        "    all_pixels = feature_extractor(images=img, return_tensors=\"np\").pixel_values\n",
        "  output_ids = model.generate(pixel_values, max_length=16, num_beams=4).sequences\n",
        "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "\n",
        "  return preds[0]\n",
        "\n",
        "image_caption = generate_caption(input_image_path, image_cap_model)\n",
        "\n",
        "print(\"Image caption is: \",image_caption)"
      ],
      "metadata": {
        "id": "Quy9qMU3R0WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Object Detection"
      ],
      "metadata": {
        "id": "BlrSCLdUUopr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we will use Mask R-CNN to detect exactly the amount of objects if input text specified, since the image caption may not contains this attribute and the framework needs double check."
      ],
      "metadata": {
        "id": "mhHO4Lo2U7bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference."
      ],
      "metadata": {
        "id": "-mI7xqLwVn9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "Z0dh3hw9VcrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then count the number of each class"
      ],
      "metadata": {
        "id": "ETJXT_cDb6Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_number_objects(img_path, predictor):\n",
        "  img = cv2.imread(img_path)\n",
        "  outputs = predictor(img)\n",
        "  instance_dict={}\n",
        "  for class_id in outputs[\"instances\"].pred_classes:\n",
        "    class_name = classes[int(class_id)]\n",
        "    instance_dict[class_name]=instance_dict.get(class_name, 0)+1\n",
        "  \n",
        "  return instance_dict\n",
        "\n",
        "instance_count = count_number_objects(input_image_path, predictor)\n",
        "\n",
        "print(\"The number of each class is: \", instance_count)"
      ],
      "metadata": {
        "id": "0Yf3x3ZvcAOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Sentence analysis"
      ],
      "metadata": {
        "id": "xPss_Y9WeWOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we will analyze the input sentence to get fully information"
      ],
      "metadata": {
        "id": "ZUikPN5Vejzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "def get_word_dict(text):\n",
        "  # load english language model\n",
        "  nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
        "\n",
        "  # create spacy \n",
        "  doc = nlp(text)\n",
        "\n",
        "  word_dict={}\n",
        "  for token in doc:\n",
        "    # extract subject\n",
        "    if (token.pos_=='NOUN'):\n",
        "        num=1\n",
        "        for child in token.children:\n",
        "          if child.dep_=='nummod':\n",
        "            num=int(child.text)\n",
        "        word_dict[token.lemma_]=num\n",
        "\n",
        "  return word_dict\n",
        "\n",
        "word_dict = get_word_dict(input_text)\n",
        "\n",
        "print(\"The number of object required: \", word_dict)"
      ],
      "metadata": {
        "id": "l73xQy_FgAde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we also need to get the embedd vectors of both Input Text Prompt and Image Caption by using LaBSE"
      ],
      "metadata": {
        "id": "QmKshgzGkArB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentence embed model\n",
        "sentence_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
        "\n",
        "# encode sentences to vectors\n",
        "input_embed = sentence_model.encode(input_text)\n",
        "caption_embed = sentence_model.encode(image_caption)"
      ],
      "metadata": {
        "id": "fDv_2PxhnBWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Score"
      ],
      "metadata": {
        "id": "PZQTor6YqZYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now base on all the information we have, we can compute the final score."
      ],
      "metadata": {
        "id": "gsEq4I9Kppoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation on the number of objects\n",
        "def score_objects_num(word_dict, instance_dict):\n",
        "  class_scores=[]\n",
        "  for class_name in instance_dict.keys():\n",
        "    if class_name in word_dict:\n",
        "      class_scores.append(1-abs(instance_dict[class_name]-word_dict[class_name])/word_dict[class_name])\n",
        "\n",
        "  return np.mean(class_scores)\n",
        "\n",
        "\n",
        "# evaluation on sentence vector simi\n",
        "def score_sentence_similarity(input_embed, caption_embed):\n",
        "  similarity = torch.cosine_similarity(torch.tensor(input_embed), torch.tensor(caption_embed), dim=0)\n",
        "  \n",
        "  return similarity\n",
        "\n",
        "# map score to range (0,5)\n",
        "def map(score):\n",
        "  return round(score*5)\n",
        "\n",
        "score_1 = score_objects_num(word_dict, instance_dict)\n",
        "score_2 = score_sentence_similarity(input_embed, caption_embed)\n",
        "\n",
        "score = map(np.mean(score_1, score_2))\n",
        "\n",
        "print(\"The Final Score is: \", score)"
      ],
      "metadata": {
        "id": "ONRPM9FLrpc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}